<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Yen-Yu Chang</title>
  <meta name="description" lang="en" content="This is an academic website for Yen-Yu Chang to share his experiences, projects, publications.">
  <meta name="description" lang="cn" content="這是張晏祐的學術個人網站，主要分享一些他的學術論文和壹些曾經做過的專案．">
  <meta name="keywords" lang="en" content="Yen-Yu Chang,Publications,Posts,Research,Deep Learning, Graph Learning, Network Analysis, Time Series Analysis." />
  <meta name="keywords" lang="cn" content="張晏祐" />
  
  

  <link rel="shortcut icon" href="/images/favicon.svg">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="custom.css">
  <link rel="canonical" href="https://yuyuchang.github.io/">
  <link rel="alternate" type="application/rss+xml" title="Yen-Yu Chang" href="https://yuyuchang.github.io/feed.xml" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <script src="/js/jquery-2.1.3.min.js"> </script>

  <!--[if lt IE 9]>
<script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/r29/html5.min.js">
</script>
<![endif]-->



</head>


  <body>
    <!-- Google Tag Manager -->
<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-K5ZTMW"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-K5ZTMW');</script>
<!-- End Google Tag Manager -->

    <header class="site-header">

  <div class="wrapper">
    <a class="site-title" href="/">Yen-Yu Chang</a>
<nav class="site-nav">
  <a href="#" class="menu-icon">
    <svg viewBox="0 0 18 15">
      <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
	  <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
	  <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
    </svg>
  </a>

  <div class="trigger">
    <a class="page-link" href="/">Home</a>

    
      <!-- Button for Posts -->
      

    
      <!-- Button for Posts -->
      

    
      <!-- Button for Posts -->
      
        
        <a class="page-link" href="#timeline">Experiences</a>
		<a class="page-link" href="#publications">Publications</a>
		<a class="page-link" href="#awards">Honors & Awards</a>
        
      

    
      <!-- Button for Posts -->
      

    
      <!-- Button for Posts -->
      

    
      <!-- Button for Posts -->
      

    
      <!-- Button for Posts -->
      

    
      <!-- Button for Posts -->
      

    
      <!-- Button for Posts -->
      

    
      <!-- Button for Posts -->
      
        
        
      

    
      <!-- Button for Posts -->
      

    
  </div>
</nav>

  </div>
</header>

    <div id="profile-cover" class="cover shallow-bg img-responsive">
  
  <div id="profile-namecard" class="profile-wrapper wrapper-light">
    <div id="my-pic" class="profile-col profile-col-1">
      <img id="profile-avatar" src="/images/self-portrait/me.jpg" alt="Me" class="circle-img border-dark"/>
    </div>
    <div id="my-contact" class="profile-col profile-col-2">
      <div id="my-name" class="text-grey-dark">
        Yen-Yu Chang<br>
      </div>
      <div id="my-title" class="text-grey">
        Research Engineer <br> @ Horizon Robotics
      </div>
      <div id="my-email" class="text-grey-light">
        yenyu [at] cs.stanford.edu
      </div>
      <div class="social-media">


<a href="https://scholar.google.com/citations?user=oyxAFOsAAAAJ&hl=zh-TW" class="icon-button github">
  <i class="fa fa-graduation-cap icon-github"></i>
  <span></span>
</a>

<a href="https://github.com/yuyuchang" class="icon-button github">
  <i class="fa fa-github icon-github"></i>
  <span></span>
</a>


<!--
<a href="https://twitter.com/Hexiang_Hu" class="icon-button twitter">
  <i class="fa fa-twitter icon-twitter"></i>
  <span></span>
</a>



<a href="https://facebook.com/Hu.Hexiang" class="icon-button facebook">
  <i class="fa fa-facebook icon-facebook"></i>
  <span></span>
</a>
-->


<a href="https://linkedin.com/in/yenyuchang" class="icon-button linkedin">
  <i class="fa fa-linkedin icon-linkedin"></i>
  <span></span>
</a>

<a href="/assets/resume/resume.pdf" class="icon-button">
  <i class="fa">
    <img class="social-icon" src="/images/icon/cv-symbol.png"/>
  </i>
  <span></span>
</a>

      </div>
    </div>
    <div id="my-desc" class="profile-col profile-col-2 hide">
      <div id="my-desc-title">
        Another Me
      </div>
      <div id="my-desc-content">
        I love sports, especially basketball and table tennis.
      </div>
    </div>
  </div>
</div>
<script type="text/javascript">
  function deepFeature()
  {
    $('#profile-avatar').attr('src', '/images/self-portrait/deep-me-2.jpg');
    $('#profile-avatar').removeClass('border-dark');
    $('#profile-avatar').addClass('border-bright');

    $('#profile-cover').removeClass('shallow-bg');
    $('#profile-cover').addClass('deep-bg');
    
    $('#profile-namecard').removeClass('wrapper-light');
    $('#profile-namecard').addClass('wrapper-dark');
    
    $('#my-desc').removeClass('hide');
    $('#my-contact').addClass('hide');    
    // console.log('Move in now...');
  }

  function shallowFeature()
  {
    $('#profile-avatar').attr('src', '/images/self-portrait/me.jpg');
    $('#profile-avatar').removeClass('border-bright');
    $('#profile-avatar').addClass('border-dark');

    $('#profile-cover').removeClass('deep-bg');
    $('#profile-cover').addClass('shallow-bg');
    
    $('#profile-namecard').removeClass('wrapper-dark');
    $('#profile-namecard').addClass('wrapper-light');
    
    $('#my-contact').removeClass('hide');
    $('#my-desc').addClass('hide');
    // console.log('Move out now...');
  }

  $(function() { 
    $('#my-pic').hover(deepFeature, shallowFeature);
  })

</script>

    <div class="page-content">
      <div class="wrapper">
        <div id="bio" class="bio">
  <h1 class="md-heading text-center">
    <i class="fa fa-id-card" aria-hidden="true"></i>
    Biography
  </h1>
  <div class="bio-body" style="overflow:hidden;">
    <p>
      I am a research engineer at Horizon Robotics. I am generally interested in computer vision, object-oriented learning, multimodal learning, and reinforcement leraning. My recent research focuses on object-centric neural rendering and object-oriented representation learning and reinforcement learning. <br>
      <br>
      I received my Bachelor's degree in Electrical Engineering from National Taiwan University (NTU) in 2018, and my Master's in Electrical Engineering from Stanford University in 2021. I have had the privilege to work with <a href='https://profiles.stanford.edu/fei-fei-li'>Prof. Li Fei-Fei</a>, <a href='https://jiajunwu.com/'>Prof. Jiajun Wu</a>, <a href='https://geometry.stanford.edu/member/guibas/'>Prof. Leonidas Guibas</a>, <a href='https://cs.stanford.edu/people/jure/'>Prof. Jure Leskovec</a>, and <a href='https://sites.google.com/view/panli-uiuc/home'> Prof. Pan Li</a> at Stanford. I also had the pleasure of working with <a href='https://www.ee.ntu.edu.tw/profile1.php?id=100129'>Prof. Ho-Lin Chen</a>, <a href='https://www.csie.ntu.edu.tw/~sdlin/'> Prof. Shou-De Lin</a>, and <a href='https://speech.ee.ntu.edu.tw/~tlkagk/'> Prof. Hung-Yi Lee</a> at NTU. If you would like to learn more about me, please see my [ <a href='/assets/resume/resume.pdf'><strong>Résumé</strong></a> ] or contact me at yenyu [at] cs.stanford.edu. <br>

	  <div style="width: 50%; float:left">
	    <h3>
		  <i class="fa fa-star" aria-hidden="true"></i>
		  <strong>Interests</strong>
		</h3>
		<ul>
		  <li>Machine Learning / Deep Learning</li>
      <li>Computer Vision/ Neural Rendering</li>
      <li>Reinforcement Learning / Multimodal Learning</li>
		  <li>Structure Learning / Graph Mining</li>
		</ul>
	  </div>
	  
	  <div style="width: 40%; float:right">
	    <h3>
		  <i class="fa fa-graduation-cap" aria-hidden="true"></i>
		  <strong>Education</strong>
		</h3>
		National Taiwan University (NTU)<br>
		B.S. in Electrical Engineering, 2018 <br>
    Stanford University <br>
    M.S. in Electrical Engineering, 2021 <br>
	  </div>
  </div>
</div>


<div id="timeline" class="timeline-brief">
  <h1 class="md-heading text-center">
    <i class="fa fa-tasks" aria-hidden="true"></i>
    Timeline & Experiences
  </h1>

  <div class="timeline-body">


    <div class="timeline-item">
      <div class="timeline-date">
      <img class="social-icon" src="/images/icon/horizonrobotics.png"/>
        2021 Dec. - 
      </div>
      <div class="timeline-title">
        Research Engineer @ Horizon Robotics
      </div>
      <div class="timeline-desc">
        Representation Learning, Object-oriented Learning, & Reinforcement Learning
      </div>
      
    </div>
    
    
    <div class="timeline-item">
      <div class="timeline-date">
      <img class="social-icon" src="/images/icon/stanford.png"/>
        2021 Jul. - 2021 Nov.
      </div>
      <div class="timeline-title">
        Research Assistant @ Stanford Vision and Learning Lab (SVL)
      </div>
      <div class="timeline-desc">
        Computer Vision, Neural Rendering, and Multimodal Learning
      </div>
      
      <div class="timeline-host">
        Supervisor: <a href='https://profiles.stanford.edu/fei-fei-li'>Prof. Li Fei-Fei</a> & <a href='https://jiajunwu.com/'>Prof. Jiajun Wu</a>
      </div>
      
    </div>



    <div class="timeline-item">
      <div class="timeline-date">
	    <img class="social-icon" src="/images/icon/stanford.png"/>
        2019 Sep. - 2021 Jun.
      </div>
      <div class="timeline-title">
        EE master student @ Stanford University
      </div>
      <div class="timeline-desc">
        Research Assistant @ Stanford Network Analysis Project (SNAP) <br>
        - Deep Learning <br>
        - Graph Mining <br>
        - Anomaly Detection <br>
      </div>
      
      <div class="timeline-host">
        Supervisor: <a href='https://cs.stanford.edu/~jure/'>Prof. Jure Leskovec</a> & <a href='https://sites.google.com/view/panli-uiuc/home'>Prof. Pan Li</a>
      </div>

      <div class="timeline-desc">
        Research Assistant @ Stanford Vision & Learning Lab (SVL) <br>
        - Computer Vision <br>
        - Neural Rendering <br>
        - Multimodal Learning <br>
      </div>
      
      <div class="timeline-host">
        Supervisor: <a href='https://profiles.stanford.edu/fei-fei-li'>Prof. Li Fei-Fei</a> & <a href='https://jiajunwu.com/'>Prof. Jiajun Wu</a>
      </div>

      <div class="timeline-desc">
        Graduate Researcher @ Stanford Geometric Computation Group <br>
        - Computer Vision <br>
        - 3D Learning <br>
        - CAD Model Analysis <br>
      </div>
      
      <div class="timeline-host">
        Supervisor: <a href='https://geometry.stanford.edu/member/guibas/'>Prof. Leonidas Guibas</a>
      </div>
      
    </div>
    
    
    
    <div class="timeline-item">
      <div class="timeline-date">
	    <img class="social-icon" src="/images/icon/stanford.png"/>
        2019 Jul. - 2019 Sep.
      </div>
      <div class="timeline-title">
        Summer Research Intern @ Stanford Network Analysis Project (SNAP)
      </div>
      <div class="timeline-desc">
        
      </div>
      
      <div class="timeline-host">
        Host: <a href='https://cs.stanford.edu/~jure/'>Prof. Jure Leskovec</a> & <a href='https://cs.stanford.edu/~emmap1/'>Prof. Emma Pierson</a>
      </div>
      
    </div>
    
    <div class="timeline-item">
      <div class="timeline-date">
	    <img class="social-icon" src="/images/icon/ntu.png"/>
        2014 - 2018
      </div>
      <div class="timeline-title">
        Undergraduate student & researcher @ NTU
      </div>
      <div class="timeline-desc">
	    Electrical Engineering department<br>
		Game Theory and Molecular Computing Laboratory & Speech Processing and Machine Learning Laboratory<br>
		Primary focus: <br>
		- Network creation games <br>
		- Price of anarchy (PoA) <br>
		- Speech enhancement <br>
        Computer Science department<br>
		Machine Discovery and Social Network Mining Laboratory<br>
		Primary focus: <br>
		- (Multiagent) Reinforcement learning <br>
		- Time series prediction <br>
      </div>
      
      <div class="timeline-host">
        Supervisor: <a href='https://www.ee.ntu.edu.tw/profile1.php?id=100129'>Prof. Ho-Lin Chen</a>, <a href='https://www.csie.ntu.edu.tw/~sdlin/'> Prof. Shou-De Lin</a>, & <a href='https://speech.ee.ntu.edu.tw/~tlkagk/'> Prof. Hung-Yi Lee</a>
      </div>
      
    </div>
    
  </div>

  <div class="timeline-start">
    <div class="timeline-date">
	  1996
	</div>
	<div class="timeline-title">
	  He was born.
	</div>
  </div>

  <!--
  <div class="button">
    <h1 class="sm-heading text-center">
      <a href="/timeline/#timeline">
        <i class="fa fa-info-circle"></i>
      </a>
    </h1>
  </div>
  -->
</div>

<div id="publications" class="publications">
  <h1 class="md-heading text-center">
    <i class="fa fa-file" aria-hidden="true"></i>
    Selected Publications
  </h1>

  <div class="pub-list">


    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/publications/2022/Point2Cyl.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Point2Cyl: Reverse Engineering 3D Objects from Point Clouds to Extrusion Cylinders
        </div>
        <div class="authors">
          
            <a class="author" href="https://mikacuy.github.io/">Mikaela Angelina Uy</a>,
            <a class="author" href="https://yuyuchang.github.io/"><strong>Yen-Yu Chang</strong></a>,
            <a class="author" href="https://mhsung.github.io/">Minhyuk Sung</a>,
            <a class="author" href="https://www.purvigoel.com/">Purvi Goel</a>,
            <a class="author" href="https://www.autodesk.com/research/people/joseph-lambourne">Joseph Lambourne</a>,
            <a class="author" href="https://scholar.google.com/citations?user=_Bxd5ggAAAAJ&hl=en">Tolga Birdal</a>,
            <a class="author" href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a>
          
        </div>
        <div class="desc">
          <p>
          
            We propose Point2Cyl, a supervised network transforming a raw 3D point cloud to a set of extrusion cylinders. In this work, we introduce a neural network that solves the extrusion cylinder decomposition problem in a geometry-grounded way by first learning underlying geometric proxies. Precisely, our approach first predicts per-point segmentation, base/barrel labels and normals, then estimates for the underlying extrusion parameters in differentiable and closed-form formulations.
          
          </p>
        </div>
        <div class="publish">
          <span class="publisher">CVPR 2022</span>
          <span class="status"></span>
          <span class="place">New Orleans, LA</span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://arxiv.org/pdf/2112.09329.pdf"> <strong>arXiv</strong> </a>]
          
        </div>
      </div>
    </div>


    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/publications/2022/ObjectFolderV2.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          ObjectFolder 2.0: A Multisensory Object Dataset for Sim2Real Transfer
        </div>
        <div class="authors">
          
            <a class="author" href="https://ai.stanford.edu/~rhgao/">Ruohan Gao</a>,
            <a class="author" href="https://si-lynnn.github.io/">Zilin Si</a>,
            <a class="author" href="https://yuyuchang.github.io/"><strong>Yen-Yu Chang</strong></a>,
            <a class="author" href="https://samuelpclarke.com/">Samuel Clarke</a>,
            <a class="author" href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>,
            <a class="author" href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei</a>,
            <a class="author" href="https://www.ri.cmu.edu/ri-faculty/wenzhen-yuan/">Wenzhen Yuan</a>,
            <a class="author" href="https://jiajunwu.com/">Jiajun Wu</a>
          
        </div>
        <div class="desc">
          <p>
          
            We present ObjectFolder 2.0, a large-scale, multisensory dataset of common household objects in the form of implicit neural representations that significantly enhances ObjectFolder 1.0 in three aspects. First, our dataset is 10 times larger in the amount of objects and orders of magnitude faster in rendering time. Second, we significantly improve the multisensory rendering quality for all three modalities. Third, we show that models learned from virtual objects in our dataset successfully transfer to their real-world counterparts in three challenging tasks: object scale estimation, contact localization, and shape reconstruction.
          
          </p>
        </div>
        <div class="publish">
          <span class="publisher">CVPR 2022</span>
          <span class="status"></span>
          <span class="place">New Orleans, LA</span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://arxiv.org/pdf/2204.02389.pdf"> <strong>arXiv</strong> </a>]
          [<a class="tag" href="https://ai.stanford.edu/~rhgao/objectfolder2.0/ObjectFolderV2_Supp.pdff"> <strong>supp</strong> </a>]
          [<a class="tag" href="https://ai.stanford.edu/~rhgao/objectfolder2.0/"> <strong>project page</strong> </a>]
          [<a class="tag" href="https://github.com/rhgao/ObjectFolder"> <strong>code</strong> </a>]
          
        </div>
      </div>
    </div>


    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/publications/2021/ObjectFolderV1.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations
        </div>
        <div class="authors">
          
            <a class="author" href="https://ai.stanford.edu/~rhgao/">Ruohan Gao</a>,
            <a class="author" href="https://yuyuchang.github.io/"><strong>Yen-Yu Chang</strong></a>,
            <a class="author" href="">Shivani Mall</a>,
            <a class="author" href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei</a>,
            <a class="author" href="https://jiajunwu.com/">Jiajun Wu</a>
          
        </div>
        <div class="desc">
          <p>
          
            We present ObjectFolder, a dataset of 100 virtualized objects that addresses both challenges with two key innovations. First, ObjectFolder encodes the visual, auditory, and tactile sensory data for all objects, enabling a number of multisensory object recognition tasks, beyond existing datasets that focus purely on object geometry. Second, ObjectFolder employs a uniform, object-centric, and implicit representation for each object's visual textures, acoustic simulations, and tactile readings, making the dataset flexible to use and easy to share.
          
          </p>
        </div>
        <div class="publish">
          <span class="publisher">CoRL 2021</span>
          <span class="status"></span>
          <span class="place">(Virtual)</span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://arxiv.org/pdf/2109.07991.pdf"> <strong>arXiv</strong> </a>]
		      [<a class="tag" href="https://ai.stanford.edu/~rhgao/objectfolder/"> <strong>project page</strong> </a>]
		      [<a class="tag" href="https://github.com/rhgao/ObjectFolder"> <strong>code</strong> </a>]
          
        </div>
      </div>
    </div>
  

    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/publications/2020/CAW.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Inductive Representation Learning in Temporal Networks via Causal Anonymous Walks
        </div>
        <div class="authors">
          
            <a class="author" href="">Yanbang Wang</a>,
            <a class="author" href="https://yuyuchang.github.io/"><strong>Yen-Yu Chang</strong></a>,
            <a class="author" href="https://wenwen0319.github.io/">Yunyu Liu</a>,
            <a class="author" href="https://sites.google.com/view/panli-uiuc/home">Pan Li</a>,
            <a class="author" href="https://cs.stanford.edu/people/jure/">Jure Leskovec</a>
          
        </div>
        <div class="desc">
          <p>
          
            In this paper we propose Causal Anonymous Walks (CAWs) to inductively represent a temporal network. We further propose a neural-network model CAW-N to encode CAWs, and pair it with a CAW sampling strategy with constant memory and time cost to support online training and inference. CAW-N is evaluated to predict links over 6 real temporal networks and uniformly outperforms previous SOTA methods by averaged 15% AUC gain in the inductive setting. CAW-N also outperforms previous methods in 5 out of the 6 networks in the transductive setting.
          
          </p>
        </div>
        <div class="publish">
          <span class="publisher">ICLR 2021</span>
          <span class="status"></span>
          <span class="place">(Virtual)</span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://arxiv.org/pdf/2101.05974.pdf"> <strong>arXiv</strong> </a>]
		  [<a class="tag" href="http://snap.stanford.edu/caw/"> <strong>project page</strong> </a>]
		  [<a class="tag" href="https://github.com/snap-stanford/CAW"> <strong>code</strong> </a>]
          
        </div>
      </div>
    </div>
  
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/publications/2020/F-FADE.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          F-FADE: Frequency Factorization for Anomaly Detection in Edge Streams
        </div>
        <div class="authors">
          
            <a class="author" href="https://yuyuchang.github.io"><strong>Yen-Yu Chang</strong></a>,
            <a class="author" href="https://sites.google.com/view/panli-uiuc/home">Pan Li</a>,
            <a class="author" href="https://scholar.google.com/citations?user=xlZ4YJcAAAAJ&hl=en&oi=ao">Rok Sosic</a>,
            <a class="author" href="">M. H. Afifi</a>,
            <a class="author" href="">Marco Schweighauser</a>,
            <a class="author" href="https://cs.stanford.edu/people/jure/">Jure Leskovec</a>
          
        </div>
        <div class="desc">
          <p>
          
		    we propose F-FADE, a new approach for detection of anomalies in edge streams, which uses a novel frequency-factorization technique to efficiently model the time-evolving distributions of frequencies of interactions between node-pairs. Our experiments on one synthetic and six real-world dynamic networks show that F-FADE achieves state of the art performance and may detect anomalies that previous methods are unable to find.
          
          </p>
        </div>
        <div class="publish">
          <span class="publisher">WSDM 2021</span>
          <span class="status"></span>
          <span class="place">(Virtual)</span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://arxiv.org/abs/2011.04723"> <strong>arXiv</strong> </a>]
          [<a class="tag" href="http://snap.stanford.edu/f-fade/"> <strong>project page</strong> </a>]
          [<a class="tag" href="https://github.com/snap-stanford/F-FADE"> <strong>code</strong> </a>]
		  [<a class="tag" href="https://yuyuchang.github.io/posters/WSDM_21_F_FADE_Poster.jpeg"> <strong>poster</strong> </a>]
		  [<a class="tag" href="https://vimeo.com/518922175"> <strong>video</strong> </a>]
          
        </div>
      </div>
    </div>
  
  
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/publications/2019/regulation.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          A Regulation Enforcement Solution for Multi-agent Reinforcement Learning
        </div>
        <div class="authors">
          <a class="author" href="https://fanyun-sun.github.io/">Fan-Yun Sun</a>,
          <a class="author" href="https://yuyuchang.github.io"><strong>Yen-Yu Chang</strong></a>,
          <a class="author" href="https://kristery.github.io/">Yueh-Hua Wu</a>,
          <a class="author" href="https://www.csie.ntu.edu.tw/~sdlin/">Shou-De Lin</a>              
        </div>
        <div class="desc">
          <p>
          
            In this paper, we proposed a framework to solve the following problem: In a decentralized environment, given that not all agents are compliant to regulations at first, can we develop a mechanism such that it is in the self-interest of non-compliant agents to comply after all. We utilized empirical game-theoretic analysis to justify our method.
          
          </p>
        </div>
        <div class="publish">
          <span class="publisher">AAMAS 2019</span>
          <span class="status"></span>
          <span class="place">Montreal, QC</span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://arxiv.org/abs/1901.10059"> <strong>arXiv</strong> </a>]
          
          [<a class="tag" href="https://drive.google.com/open?id=1jtjyK0uv31V39hzrRzXzsXdx5QxvmKV98b_NEw2fkh4"> <strong>slides</strong> </a>]
          
        </div>
      </div>
    </div>
  
  
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/publications/2018/diminishing.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Designing Non-greedy Reinforcement Learning Agents with Diminishing Reward Shaping
        </div>
        <div class="authors">
		  <a class="author" href="https://fanyun-sun.github.io/">Fan-Yun Sun</a>,
          <a class="author" href="https://yuyuchang.github.io"><strong>Yen-Yu Chang</strong></a>,
          <a class="author" href="https://kristery.github.io/">Yueh-Hua Wu</a>,
          <a class="author" href="https://www.csie.ntu.edu.tw/~sdlin/">Shou-De Lin</a>
        </div>
        <div class="desc">
          <p>
		    This paper intends to address an issue in multi-agent RL that when agents possessing varying capabilities. We introduce a simple method to train non-greedy agents with nearly no extra cost. Our model can achieve the following goals: non-homogeneous equality, only need local information, cost-effective, generalizable and configurable.
          </p>
        </div>
        <div class="publish">
          <span class="publisher">AAAI/ACM conference on AI, Ethics, Society 2018</span>
          <span class="status">(Oral)</span>
          <span class="place">New Orleans, LA</span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_97.pdf"> <strong>pdf</strong> </a>]
          
          [<a class="tag" href="https://docs.google.com/presentation/d/1WENgm81DSKxCf2jgXUSgLKWgeUfZSix7F7MDOKNLbUc/edit?usp=sharing"> <strong>slides</strong> </a>]
          
        </div>
      </div>
    </div>
  
  
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/publications/preprint/memory.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          A Memory-Network Based Solution for Multivariate Time-Series Forecasting
        </div>
        <div class="authors">
          <a class="author" href="https://yuyuchang.github.io"><strong>Yen-Yu Chang</strong></a>,
          <a class="author" href="https://fanyun-sun.github.io/">Fan-Yun Sun</a>,
          <a class="author" href="https://kristery.github.io/">Yueh-Hua Wu</a>,
          <a class="author" href="https://www.csie.ntu.edu.tw/~sdlin/">Shou-De Lin</a>          
        </div>
        <div class="desc">
          <p>
            Inspired by Memory Network for solving the question-answering tasks, we proposed a deep learning based model named Memory Time-series network (MTNet) for time series forecasting. Additionally, the attention mechanism designed enable MTNet to be interpretable.          
          </p>
        </div>
        <div class="publish">
          <span class="publisher">preprint</span>
          <span class="status"></span>
          <span class="place"></span>
        </div>
        <div class="tags">
		  [<a class="tag" href="https://arxiv.org/abs/1809.02105"> <strong>arXiv</strong> </a>]
        </div>
      </div>
    </div>
  
  
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/publications/preprint/ans.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
		  ANS: Adaptive Network Scaling for Deep Rectifier Reinforcement Learning Models
        </div>
        <div class="authors">
		  <a class="author" href="https://kristery.github.io/">Yueh-Hua Wu</a>,
          <a class="author" href="https://fanyun-sun.github.io/">Fan-Yun Sun</a>,
          <a class="author" href="https://yuyuchang.github.io"><strong>Yen-Yu Chang</strong></a>,
          <a class="author" href="https://www.csie.ntu.edu.tw/~sdlin/">Shou-De Lin</a>
        </div>
        <div class="desc">
          <p>
		    This work provides a thorough study on how reward scaling can affect performance of deep reinforcement learning agents. We also propose an Adaptive Network Scaling framework to find a suitable scale of the rewards during learning for better performance. We conducted empirical studies to justify the solution.
          </p>
        </div>
        <div class="publish">
          <span class="publisher">preprint</span>
          <span class="status"></span>
          <span class="place"></span>
        </div>
        <div class="tags">
		  [<a class="tag" href="https://arxiv.org/abs/1809.02112"> <strong>arXiv</strong> </a>]
        </div>
      </div>
    </div>
  
  
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/publications/preprint/celebrity.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Heterogeneous Star Celebrity Games
        </div>
        <div class="authors">
          
            <a class="author" href="https://yuyuchang.github.io/"><strong>Yen-Yu Chang</strong></a>,
            <a class="author" href="">Chin-Chia Hsu</a>,
            <a class="author" href="https://www.ee.ntu.edu.tw/profile1.php?id=100129">Ho-Lin Chen</a>

		</div>
        <div class="desc">
          <p>
		    In this paper, we study the problem of heterogeneous star celebrity games. We prove that the PoA is upper bounded by O(n/β) for all heterogeneous star celebrity games. The bound is asymptotically tight even when restricted to the max celebrity game model and matches with the upper bound on the star celebrity game model. We also show that this upper bound is tight for an extension of the bounded distance network creation games.
          </p>
        </div>
        <div class="publish">
          <span class="publisher">preprint</span>
          <span class="status"></span>
          <span class="place"></span>
        </div>
        <div class="tags">
          [<a class="tag" href="/papers/Celebrity_games_with_Heterogeneous_Preferences.pdf"> <strong>pdf</strong> </a>]
        </div>
      </div>
    </div>  

  </div>

  <div class="button">
    <h1 class="sm-heading text-center">
      <!--
      <a href="/publications/#publications">
        <i class="fa fa-info-circle"></i> All Publications
      </a>
    -->
    </h1>
  </div>
</div>

<div id="awards" class="awards">
  <h1 class="md-heading text-center">
    <i class="fa fa-trophy" aria-hidden="true"></i>
    Honors & Awards
  </h1>

                <div class="bio">
                        <!--<button type="button" class="btn btn-default btn-xs" onclick="showpub(0)">All</button>-->
                        <!--<button type="button" class="btn btn-default btn-xs" onclick="showpub(1)">International</button>-->
                        <!--<button type="button" class="btn btn-default btn-xs" onclick="showpub(2)">Domestic</button>-->

                <ul>
                        <h3 class="grey-hl"><strong>International</strong></h3>

    <li>
            Ranked 19th (out of 4180) / <strong><a href="https://biendata.com/competition/kdd_2018/">KDD CUP</a> - Main Track</strong> / 2018
    </li>
    <li>
      Ranked 4th (out of 4180) / <strong>KDD CUP - Specialized Prize for long term prediction</strong> / 2018
    </li>

    </ul>

    <ul>
            <h3 class="grey-hl"><strong>Domestic</strong></h3>
      <li>
      Dean's List / <strong>National Taiwan University</strong> / 2016
      </li>
	  <li>
      Finalist (Top 30) / <strong>International Physics Olympiad Domestic Final</strong> / 2013
	  </li>

    </ul>
                </div>

</div>

</div>

      </div>
    </div>

    
<footer class="site-footer">

  <div class="wrapper">

    <!--
    <h2 class="footer-heading">
      <a class="page-link" href="https://hexiang-hu.github.io/about">
      About Hexiang Hu
    </a></h2>
	-->

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>Yen-Yu Chang</li>
          <li><a href="mailto:">yenyu [at] cs.stanford.edu</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        
<!--
<a href="https://twitter.com/Hexiang_Hu">
  <img src="/images/icon/color-twitter.png" class="social-icon">
</a>
-->

<a href="https://www.linkedin.com/in/yenyuchang">
  <img src="/images/icon/color-linkedin.png" class="social-icon">
</a>


<a href="https://github.com/yuyuchang">
  <img src="/images/icon/color-github.png" class="social-icon">
</a>


<!--
<a href="https://facebook.com/Hu.Hexiang">
  <img src="/images/icon/color-facebook.png" class="social-icon">
</a>

<a href="/feed.xml">
  <img src="/images/icon/color-rss.png" class="social-icon">
</a>
-->


      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">This is an academic website for Yen-Yu Chang to share his experiences, projects, publications. The style of this website is borrowed from <a href='http://hexianghu.com/'>Hexiang(Frank) Hu's</a>.</p>
      </div>
    </div>

  </div>

</footer>


    <div class="back-to-top">Top</div>


<script type="text/javascript">
jQuery(document).ready(function() {
    var offset = 220;
    var duration = 500;
    jQuery(window).scroll(function() {
        if (jQuery(this).scrollTop() > offset) {
            jQuery('.back-to-top').fadeIn(duration);
        } else {
            jQuery('.back-to-top').fadeOut(duration);
        }
    });
    
    jQuery('.back-to-top').click(function(event) {
        event.preventDefault();
        jQuery('html, body').animate({scrollTop: 0}, duration);
        return false;
    })
});
</script>
  </body>
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60071442-1', 'auto');
  ga('send', 'pageview');

</script>

</html>
