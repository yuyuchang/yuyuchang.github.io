<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Yen-Yu Chang</title>
  <meta name="description" lang="en" content="This is an academic website for Yen-Yu Chang to share his experiences, projects, publications.">
  <meta name="description" lang="cn" content="這是張晏祐的學術個人網站，主要分享一些他的學術論文和壹些曾經做過的專案．">
  <meta name="keywords" lang="en" content="Yen-Yu Chang,Publications,Posts,Research,Deep Learning, Graph Learning, Network Analysis, Time Series Analysis." />
  <meta name="keywords" lang="cn" content="張晏祐" />
  
  

  <link rel="shortcut icon" href="/images/favicon.svg">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://yuyuchang.github.io/">
  <link rel="alternate" type="application/rss+xml" title="Yen-Yu Chang" href="https://yuyuchang.github.io/feed.xml" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <script src="/js/jquery-2.1.3.min.js"> </script>

  <!--[if lt IE 9]>
<script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/r29/html5.min.js">
</script>
<![endif]-->



</head>


  <body>
    <!-- Google Tag Manager -->
<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-K5ZTMW"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-K5ZTMW');</script>
<!-- End Google Tag Manager -->

    <header class="site-header">

  <div class="wrapper">
    <a class="site-title" href="/">Yen-Yu Chang</a>
<nav class="site-nav">
  <a href="#" class="menu-icon">
    <svg viewBox="0 0 18 15">
      <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
	  <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
	  <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
    </svg>
  </a>

  <div class="trigger">
    <a class="page-link" href="/">Home</a>

    
      <!-- Button for Posts -->
      

    
      <!-- Button for Posts -->
      

    
      <!-- Button for Posts -->
      
        
        <a class="page-link" href="#timeline">Experiences</a>
		<a class="page-link" href="#publications">Publications</a>
		<a class="page-link" href="#awards">Honors & Awards</a>
        
      

    
      <!-- Button for Posts -->
      

    
      <!-- Button for Posts -->
      

    
      <!-- Button for Posts -->
      

    
      <!-- Button for Posts -->
      

    
      <!-- Button for Posts -->
      

    
      <!-- Button for Posts -->
      

    
      <!-- Button for Posts -->
      
        
        
      

    
      <!-- Button for Posts -->
      

    
  </div>
</nav>

  </div>
</header>

    <div id="profile-cover" class="cover shallow-bg img-responsive">
  
  <div id="profile-namecard" class="profile-wrapper wrapper-light">
    <div id="my-pic" class="profile-col profile-col-1">
      <img id="profile-avatar" src="/images/self-portrait/me.jpg" alt="Me" class="circle-img border-dark"/>
    </div>
    <div id="my-contact" class="profile-col profile-col-2">
      <div id="my-name" class="text-grey-dark">
        Yen-Yu Chang<br>
      </div>
      <div id="my-title" class="text-grey">
        Master Student at Stanford
      </div>
      <div id="my-email" class="text-grey-light">
        yenyu [at] stanford.edu
      </div>
      <div class="social-media">


<a href="https://scholar.google.com/citations?user=oyxAFOsAAAAJ&hl=zh-TW" class="icon-button github">
  <i class="fa fa-graduation-cap icon-github"></i>
  <span></span>
</a>

<a href="https://github.com/yuyuchang" class="icon-button github">
  <i class="fa fa-github icon-github"></i>
  <span></span>
</a>


<!--
<a href="https://twitter.com/Hexiang_Hu" class="icon-button twitter">
  <i class="fa fa-twitter icon-twitter"></i>
  <span></span>
</a>



<a href="https://facebook.com/Hu.Hexiang" class="icon-button facebook">
  <i class="fa fa-facebook icon-facebook"></i>
  <span></span>
</a>
-->


<a href="https://linkedin.com/in/yenyuchang" class="icon-button linkedin">
  <i class="fa fa-linkedin icon-linkedin"></i>
  <span></span>
</a>

<a href="/file/CV.pdf" class="icon-button">
  <i class="fa">
    <img class="social-icon" src="/images/icon/cv-symbol.png"/>
  </i>
  <span></span>
</a>

      </div>
    </div>
    <div id="my-desc" class="profile-col profile-col-2 hide">
      <div id="my-desc-title">
        Another Me
      </div>
      <div id="my-desc-content">
        I love sports, especially basketball and table tennis.
      </div>
    </div>
  </div>
</div>
<script type="text/javascript">
  function deepFeature()
  {
    $('#profile-avatar').attr('src', '/images/self-portrait/deep-me.jpg');
    $('#profile-avatar').removeClass('border-dark');
    $('#profile-avatar').addClass('border-bright');

    $('#profile-cover').removeClass('shallow-bg');
    $('#profile-cover').addClass('deep-bg');
    
    $('#profile-namecard').removeClass('wrapper-light');
    $('#profile-namecard').addClass('wrapper-dark');
    
    $('#my-desc').removeClass('hide');
    $('#my-contact').addClass('hide');    
    // console.log('Move in now...');
  }

  function shallowFeature()
  {
    $('#profile-avatar').attr('src', '/images/self-portrait/me.jpg');
    $('#profile-avatar').removeClass('border-bright');
    $('#profile-avatar').addClass('border-dark');

    $('#profile-cover').removeClass('deep-bg');
    $('#profile-cover').addClass('shallow-bg');
    
    $('#profile-namecard').removeClass('wrapper-dark');
    $('#profile-namecard').addClass('wrapper-light');
    
    $('#my-contact').removeClass('hide');
    $('#my-desc').addClass('hide');
    // console.log('Move out now...');
  }

  $(function() { 
    $('#my-pic').hover(deepFeature, shallowFeature);
  })

</script>

    <div class="page-content">
      <div class="wrapper">
        <div id="bio" class="bio">
  <h1 class="md-heading text-center">
    <i class="fa fa-id-card" aria-hidden="true"></i>
    Biography
  </h1>
  <div class="bio-body">
    <p>
      <strong>Hexiang Hu</strong> is a Computer Science Ph.D. student in Viterbi School of Engineering at <strong>University of Southern California (USC)</strong>, working with <a href='http://www-bcf.usc.edu/~feisha/'>Prof. Fei Sha</a>. Prior to this, He was a Ph.D. student in Henry Samueli School of Engineering and Applied Science at <strong>University of California, Los Angeles (UCLA)</strong>. He earned his Bachelor’s degrees in Computer Science from Zhejiang University and Simon Fraser University with honor. He worked with <a href='http://www.cs.sfu.ca/~mori/'>Prof. Greg Mori</a> during his undergrads. His research interests include <strong>Machine Learning</strong>, <strong>Computer Vision</strong> and <strong>Natural Language Processing</strong>.
      [<strong>
        <a href='/assets/resume/20190806.pdf'>Résumé</a>
      </strong>]
    </p>
  </div>
</div>


<div id="timeline" class="timeline-brief">
  <h1 class="md-heading text-center">
    <i class="fa fa-tasks" aria-hidden="true"></i>
    Timeline
  </h1>

  <div class="timeline-body">
    
    
    <div class="timeline-item">
      <div class="timeline-date">
        Summer 2019
      </div>
      <div class="timeline-title">
        Intern @ Intel AI
      </div>
      <div class="timeline-desc">
        
      </div>
      
      <div class="timeline-host">
        Host: <a href='http://vladlen.info/'>Dr. Vladlen Koltun</a> & <a href='http://ozansener.net/'>Dr. Ozan Sener</a>
      </div>
      
    </div>
    
    
    
    <div class="timeline-item">
      <div class="timeline-date">
        Spring 2019
      </div>
      <div class="timeline-title">
        Visitor @ Berkeley AI Research Lab
      </div>
      <div class="timeline-desc">
        
      </div>
      
      <div class="timeline-host">
        Host: <a href='https://people.eecs.berkeley.edu/~svlevine/'>Prof. Sergey Levine</a>
      </div>
      
    </div>
    
    
    
    <div class="timeline-item">
      <div class="timeline-date">
        Summer 2018
      </div>
      <div class="timeline-title">
        Intern @ Facebook AI Research
      </div>
      <div class="timeline-desc">
        
      </div>
      
      <div class="timeline-host">
        Host: <a href='https://lvdmaaten.github.io/'>Dr. Laurens van der Maaten</a> & <a href='https://imisra.github.io/'>Dr. Ishan Misra</a>
      </div>
      
    </div>
    
    
    
    
    
    
    
    <div class="timeline-item">
      <div class="timeline-date">
        2017 - 
      </div>
      <div class="timeline-title">
        PhD student @ USC
      </div>
      <div class="timeline-desc">
        Large Scale Machine Learning, Vision and Language
      </div>
      
      <div class="timeline-host">
        Supervisor: <a href='http://www-bcf.usc.edu/~feisha/'>Prof. Fei Sha</a>
      </div>
      
    </div>
    
    
    
    <div class="timeline-item">
      <div class="timeline-date">
        2016 - 2017
      </div>
      <div class="timeline-title">
        PhD student @ UCLA
      </div>
      <div class="timeline-desc">
        Deep Learning, Vision
      </div>
      
      <div class="timeline-host">
        Supervisor: <a href='http://www-bcf.usc.edu/~feisha/'>Prof. Fei Sha</a>
      </div>
      
    </div>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  </div>

  <div class="button">
    <h1 class="sm-heading text-center">
      <a href="/timeline/#timeline">
        <i class="fa fa-info-circle"></i>
      </a>
    </h1>
  </div>
</div>

<div id="publications" class="publications">
  <h1 class="md-heading text-center">
    <i class="fa fa-file" aria-hidden="true"></i>
    Selected Publications
  </h1>

  <div class="pub-list">
  
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/placeholder.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Learning to Represent Image and Text using Denotation Graph
        </div>
        <div class="authors">
          
            <a class="author" href="http://zbwglory.github.io/">
               B. Zhang
              * 
            </a>
            ,
          
            <a class="author" href="">
               <strong>H. Hu</strong>
              * 
            </a>
            ,
          
            <a class="author" href="https://research.google/people/VihanJain/">
               V. Jain
              
            </a>
            ,
          
            <a class="author" href="https://research.google/people/author10168/">
               E. Ie
              
            </a>
            ,
          
            <a class="author" href="http://www-bcf.usc.edu/~feisha/">
               F. Sha
              
            </a>
            
          
        </div>
        <div class="desc">
          <p>
          
            In this paper, we propose learning representations from a set of implied, visually grounded expressions between image and  text, automatically mined from those datasets. In particular, we use denotation graphs to represent how specific concepts (such as sentences describing images) can be linked to abstract and generic concepts (such as short phrases) that are also visually grounded. We propose methods to incorporate such relations into learning representation.
          
          </p>
        </div>
        <div class="publish">
          <span class="publisher">EMNLP 2020</span>
          <span class="status"></span>
          <span class="place">(Virtual)</span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://arxiv.org/abs/2010.02949"> <strong>arXiv</strong> </a>]
          
          [<a class="tag" href=""> <strong>code</strong> </a>]
          
        </div>
      </div>
    </div>
  
  
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/2020/ye2020gfsl.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Learning Adaptive Classifiers Synthesis for Generalized Few-Shot Learning
        </div>
        <div class="authors">
          
            <a class="author" href="http://www.lamda.nju.edu.cn/yehj">
               H.-J. Ye
              * 
            </a>
            ,
          
            <a class="author" href="">
               <strong>H. Hu</strong>
              * 
            </a>
            ,
          
            <a class="author" href="">
               D.-C. Zhan
              
            </a>
            
          
        </div>
        <div class="desc">
          <p>
          
            We investigate the problem of generalized few-shot learning (GFSL), using dictionary based classifier synthesis.
          
          </p>
        </div>
        <div class="publish">
          <span class="publisher">To appear at IJCV</span>
          <span class="status"></span>
          <span class="place"></span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://arxiv.org/abs/1906.02944"> <strong>arXiv</strong> </a>]
          
          [<a class="tag" href="https://github.com/Sha-Lab/CASTLE"> <strong>code</strong> </a>]
          
        </div>
      </div>
    </div>
  
  
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/2020/zhu2020babywalk.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps
        </div>
        <div class="authors">
          
            <a class="author" href="https://billzhu.me/">
               W. Zhu
              * 
            </a>
            ,
          
            <a class="author" href="">
               <strong>H. Hu</strong>
              * 
            </a>
            ,
          
            <a class="author" href="http://jcchen.me/">
               J. Chen
              
            </a>
            ,
          
            <a class="author" href="http://www.sfu.ca/~zhiweid">
               Z. Deng
              
            </a>
            ,
          
            <a class="author" href="https://research.google/people/VihanJain/">
               V. Jain
              
            </a>
            ,
          
            <a class="author" href="https://research.google/people/author10168/">
               E. Ie
              
            </a>
            ,
          
            <a class="author" href="http://www-bcf.usc.edu/~feisha/">
               F. Sha
              
            </a>
            
          
        </div>
        <div class="desc">
          <p>
          
            We propose BabyWalk, a novel navigation agent that learns navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially.
          
          </p>
        </div>
        <div class="publish">
          <span class="publisher">ACL 2020</span>
          <span class="status"></span>
          <span class="place">in Seattle, WA</span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://arxiv.org/abs/2005.04625"> <strong>arXiv</strong> </a>]
          
          [<a class="tag" href="https://github.com/Sha-Lab/babywalk"> <strong>code</strong> </a>]
          
        </div>
      </div>
    </div>
  
  
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/2020/ye2020feat.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Few-Shot Learning via Embedding Adaptation with Set-to-Set Functions
        </div>
        <div class="authors">
          
            <a class="author" href="http://www.lamda.nju.edu.cn/yehj">
               H.-J. Ye
              
            </a>
            ,
          
            <a class="author" href="">
               <strong>H. Hu</strong>
              
            </a>
            ,
          
            <a class="author" href="">
               D.-C. Zhan
              
            </a>
            ,
          
            <a class="author" href="http://www-bcf.usc.edu/~feisha/">
               F. Sha
              
            </a>
            
          
        </div>
        <div class="desc">
          <p>
          
            We propose a novel approach to adapt the instance embeddings to the target classification task with a set-to-set function, yielding embeddings that are task-specific and discriminative.
          
          </p>
        </div>
        <div class="publish">
          <span class="publisher">CVPR 2020</span>
          <span class="status"></span>
          <span class="place">in Seattle, WA</span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://arxiv.org/abs/1812.03664"> <strong>arXiv</strong> </a>]
          
          [<a class="tag" href="https://github.com/Sha-Lab/FEAT"> <strong>code</strong> </a>]
          
        </div>
      </div>
    </div>
  
  
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/2019/vuorio2019mumomaml.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation
        </div>
        <div class="authors">
          
            <a class="author" href="https://vuoristo.github.io/">
               R. Vuorio
              
            </a>
            ,
          
            <a class="author" href="https://shaohua0116.github.io/">
               S.-H. Sun
              
            </a>
            ,
          
            <a class="author" href="">
               <strong>H. Hu</strong>
              
            </a>
            ,
          
            <a class="author" href="https://viterbi-web.usc.edu/~limjj/">
               J. Lim
              
            </a>
            
          
        </div>
        <div class="desc">
          <p>
          
            One important limitation of MAML is that they seek a common initialization shared across tasks, which made it suffering from adapting tasks of a multimodal distribution. This paper propose a generic method that augment MAML with the capability of identifying the task mode using a model based learner, such that it can adapt quickly with a few gradient updates.
          
          </p>
        </div>
        <div class="publish">
          <span class="publisher">NeurIPS 2019</span>
          <span class="status">(Spotlight)</span>
          <span class="place">in Vancouver, BC</span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="http://papers.nips.cc/paper/8296-multimodal-model-agnostic-meta-learning-via-task-aware-modulation"> <strong>paper</strong> </a>]
          
          [<a class="tag" href="/bib/vuorio2019multimodal.bib"> <strong>bib</strong> </a>]
          
          [<a class="tag" href="https://slideslive.com/38922982/guided-similarity-separation-for-image-retrieval"> <strong>talk</strong> </a>]
          
        </div>
      </div>
    </div>
  
  
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/2019/hexiang2019bison.jpg"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Binary Image Selection (BISON): Interpretable Evaluation of Visual Grounding
        </div>
        <div class="authors">
          
            <a class="author" href="">
               <strong>H. Hu</strong>
              
            </a>
            ,
          
            <a class="author" href="https://imisra.github.io">
               I. Misra
              
            </a>
            ,
          
            <a class="author" href="http://lvdmaaten.github.io/">
               L. van der Maaten
              
            </a>
            
          
        </div>
        <div class="desc">
          <p>
          
            This paper presents an alternative evaluation task for visual-grounding systems: given a caption the system is asked to select the image that best matches the caption from a pair of semantically similar images. The system's accuracy on this Binary Image SelectiON (BISON) task is not only interpretable, but also measures the ability to relate fine-grained text content in the caption to visual content in the images.
          
          </p>
        </div>
        <div class="publish">
          <span class="publisher">ICCV 2019 Workshop (CLVL)</span>
          <span class="status"></span>
          <span class="place">in Seoul, Korean</span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://arxiv.org/abs/1901.06595"> <strong>arXiv</strong> </a>]
          
          [<a class="tag" href="/bib/hu2019bison.bib"> <strong>bib</strong> </a>]
          
          [<a class="tag" href="/bison"> <strong>website</strong> </a>]
          
          [<a class="tag" href="https://github.com/facebookresearch/binary-image-selection"> <strong>code</strong> </a>]
          
        </div>
      </div>
    </div>
  
  
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/2019/kurt2019engaging.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Engaging Image Captioning Via Personality
        </div>
        <div class="authors">
          
            <a class="author" href="">
               K. Shuster
              
            </a>
            ,
          
            <a class="author" href="">
               S. Humeau
              
            </a>
            ,
          
            <a class="author" href="">
               <strong>H. Hu</strong>
              
            </a>
            ,
          
            <a class="author" href="">
               A. Bordes
              
            </a>
            ,
          
            <a class="author" href="">
               J. Weston
              
            </a>
            
          
        </div>
        <div class="desc">
          <p>
          
            We define a new task, Personality-Captions, where the goal is to be as engaging to humans as possible by incorporating controllable style and personality traits. We collect and release a large dataset of 201,858 of such captions conditioned over 215 possible traits.
          
          </p>
        </div>
        <div class="publish">
          <span class="publisher">CVPR 2019</span>
          <span class="status"></span>
          <span class="place">in Long Beach, CA</span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="https://arxiv.org/abs/1810.10665"> <strong>arXiv</strong> </a>]
          
          [<a class="tag" href="/bib/shuster2019engaging.bib"> <strong>bib</strong> </a>]
          
        </div>
      </div>
    </div>
  
  
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/2018/hexiang2018synpo.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Synthesized Policies for Transfer and Adaptation across Tasks and Environments
        </div>
        <div class="authors">
          
            <a class="author" href="">
               <strong>H. Hu</strong>
              * 
            </a>
            ,
          
            <a class="author" href="https://sites.google.com/usc.edu/liyuc">
               L. Chen
              * 
            </a>
            ,
          
            <a class="author" href="http://boqinggong.info/">
               B. Gong
              
            </a>
            ,
          
            <a class="author" href="http://www-bcf.usc.edu/~feisha/">
               F. Sha
              
            </a>
            
          
        </div>
        <div class="desc">
          <p>
          
            In this paper, we consider the problem of learning to simultaneously transfer across both environments (ENV) and tasks (TASK), probably more importantly, by learning from only sparse (ENV, TASK) pairs out of all possible combinations. We propose a compositional neural network which depicts a meta rule for composing policies from the environment and task embeddings.
          
          </p>
        </div>
        <div class="publish">
          <span class="publisher">NIPS 2018</span>
          <span class="status">(Spotlight)</span>
          <span class="place">in Montreal, QC</span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="/pdf/hu2018synpo.pdf"> <strong>pdf</strong> </a>]
          
          [<a class="tag" href="/pdf/hu2018synpo-supp.pdf"> <strong>supp</strong> </a>]
          
          [<a class="tag" href="/bib/hu2018synpo.bib"> <strong>bib</strong> </a>]
          
          [<a class="tag" href="/pdf/posters/nips2018_synpo.pdf"> <strong>poster</strong> </a>]
          
          [<a class="tag" href="https://sites.google.com/view/neurips2018-synpo/home"> <strong>details</strong> </a>]
          
          [<a class="tag" href="https://github.com/Sha-Lab/SynPo"> <strong>code</strong> </a>]
          
        </div>
      </div>
    </div>
  
  
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img " src="/images/projects/2015/hexiangh2015sinn.jpg"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Learning Structured Inference Neural Networks with Label Relations
        </div>
        <div class="authors">
          
            <a class="author" href="">
               <strong>H. Hu</strong>
              
            </a>
            ,
          
            <a class="author" href="http://www.cs.sfu.ca/~gza11/personal/">
               G.-T. Zhou
              
            </a>
            ,
          
            <a class="author" href="http://www.sfu.ca/~zhiweid">
               Z. Deng
              
            </a>
            ,
          
            <a class="author" href="http://web.engr.illinois.edu/~liao17/">
               Z. Liao
              
            </a>
            ,
          
            <a class="author" href="https://www.cs.sfu.ca/~mori/">
               G. Mori
              
            </a>
            
          
        </div>
        <div class="desc">
          <p>
          
            We propose a generic structured model that leverages diverse label relations to improve image classification performance. It employs a novel stacked label prediction neural network, capturing both inter-level and intra-level label semantics. The design of this framework naurally extends to leverage partial observations in the label space to inference the rest label space.
          
          </p>
        </div>
        <div class="publish">
          <span class="publisher">CVPR 2016 & T-PAMI</span>
          <span class="status"></span>
          <span class="place"></span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="http://arxiv.org/abs/1511.05616"> <strong>arXiv</strong> </a>]
          
          [<a class="tag" href="/pdf/hexiang2015sinn.pdf"> <strong>pdf</strong> </a>]
          
          [<a class="tag" href="https://ieeexplore.ieee.org/abstract/document/8613870"> <strong>T-PAMI extension</strong> </a>]
          
          [<a class="tag" href="/projects/sinn"> <strong>project</strong> </a>]
          
          [<a class="tag" href="/bib/hu2016learning.bib"> <strong>bib</strong> </a>]
          
        </div>
      </div>
    </div>
  
  
  
  
  
  
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/2017/hexiang2017negative.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets
        </div>
        <div class="authors">
          
            <a class="author" href="http://www-scf.usc.edu/~weilunc/index.html">
               W.-L. Chao
              * 
            </a>
            ,
          
            <a class="author" href="">
               <strong>H. Hu</strong>
              * 
            </a>
            ,
          
            <a class="author" href="http://www-bcf.usc.edu/~feisha/">
               F. Sha
              
            </a>
            
          
        </div>
        <div class="desc">
          <p>
          
            We show the design of the decoy answers has a significant impact on how and what the learning models learn from the datasets. In particular, the resulting learner can ignore the visual information, the question, or the both while still doing well on the task. 
          
          </p>
        </div>
        <div class="publish">
          <span class="publisher">NAACL-HLT 2018</span>
          <span class="status">(Oral)</span>
          <span class="place">in New Orleans, Louisiana</span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="/pdf/hu2017negative.pdf"> <strong>pdf</strong> </a>]
          
          [<a class="tag" href="http://www.teds.usc.edu/website_vqa"> <strong>project</strong> </a>]
          
          [<a class="tag" href="/pdf/negative2018naacl.pdf"> <strong>slides</strong> </a>]
          
          [<a class="tag" href="https://vimeo.com/276455887"> <strong>video</strong> </a>]
          
          [<a class="tag" href="/bib/chao2017being.bib"> <strong>bib</strong> </a>]
          
        </div>
      </div>
    </div>
  
  
  
    <div class="pub">
      <div class="pub-left">
        <img class="intro-img frame" src="/images/projects/2018/hexiang2018ansemb.png"/>
      </div>
      <div class="pub-right">
        <div class="title">
          Learning Answer Embedding for Visual Question Answering
        </div>
        <div class="authors">
          
            <a class="author" href="">
               <strong>H. Hu</strong>
              * 
            </a>
            ,
          
            <a class="author" href="http://www-scf.usc.edu/~weilunc/index.html">
               W.-L. Chao
              * 
            </a>
            ,
          
            <a class="author" href="http://www-bcf.usc.edu/~feisha/">
               F. Sha
              
            </a>
            
          
        </div>
        <div class="desc">
          <p>
          
            We propose a novel probabilistic model for visual question answering. 
          
          </p>
        </div>
        <div class="publish">
          <span class="publisher">CVPR 2018</span>
          <span class="status"></span>
          <span class="place">in Salt Lake City, Utah</span>
        </div>
        <div class="tags">
          
          [<a class="tag" href="/pdf/hu2018learning.pdf"> <strong>pdf</strong> </a>]
          
          [<a class="tag" href="/pdf/posters/cvpr2018_answer_embeddings.pdf"> <strong>poster</strong> </a>]
          
          [<a class="tag" href="/bib/hu2018learning.bib"> <strong>bib</strong> </a>]
          
          [<a class="tag" href="https://github.com/hexiang-hu/answer_embedding"> <strong>code</strong> </a>]
          
        </div>
      </div>
    </div>
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  </div>

  <div class="button">
    <h1 class="sm-heading text-center">
      <a href="/publications/#publications">
        <i class="fa fa-info-circle"></i> All Publications
      </a>
    </h1>
  </div>
</div>

</div>

      </div>
    </div>

    
<footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">
      <a class="page-link" href="https://hexiang-hu.github.io/about">
      About Hexiang Hu
    </a></h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>Hexiang (Frank) Hu</li>
          <li><a href="mailto:">hexiang.frank.hu [at] gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        
<a href="https://twitter.com/Hexiang_Hu">
  <img src="/images/icon/color-twitter.png" class="social-icon">
</a>



<a href="https://linkedin.com/in/{{hexianghu">
  <img src="/images/icon/color-linkedin.png" class="social-icon">
</a>



<a href="https://github.com/Hexiang-Hu">
  <img src="/images/icon/color-github.png" class="social-icon">
</a>



<a href="https://facebook.com/Hu.Hexiang">
  <img src="/images/icon/color-facebook.png" class="social-icon">
</a>



<a href="/feed.xml">
  <img src="/images/icon/color-rss.png" class="social-icon">
</a>


      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">This is an academic website for Hexiang Hu to share his experiences, projects, publications and tech/non-tech posts</p>
      </div>
    </div>

  </div>

</footer>


    <div class="back-to-top">Top</div>


<script type="text/javascript">
jQuery(document).ready(function() {
    var offset = 220;
    var duration = 500;
    jQuery(window).scroll(function() {
        if (jQuery(this).scrollTop() > offset) {
            jQuery('.back-to-top').fadeIn(duration);
        } else {
            jQuery('.back-to-top').fadeOut(duration);
        }
    });
    
    jQuery('.back-to-top').click(function(event) {
        event.preventDefault();
        jQuery('html, body').animate({scrollTop: 0}, duration);
        return false;
    })
});
</script>
  </body>
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-60071442-1', 'auto');
  ga('send', 'pageview');

</script>

</html>
